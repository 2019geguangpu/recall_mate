"use client";

import { useState, useRef, useEffect } from "react";
import { Button } from "@/components/ui/button";
import type { VoiceRecord, VoiceRecordStatus } from "@/lib/types/voice-record";
import "@/lib/types/speech-recognition";

interface VoiceRecorderProps {
  onRecordComplete?: (record: VoiceRecord) => void;
  onTranscript?: (text: string) => void;
}

export function VoiceRecorder({ onRecordComplete, onTranscript }: VoiceRecorderProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [status, setStatus] = useState<VoiceRecordStatus>("completed");
  const [duration, setDuration] = useState(0);
  const [transcript, setTranscript] = useState<string>("");
  const [interimTranscript, setInterimTranscript] = useState<string>("");
  const [isSupported, setIsSupported] = useState(false);
  const [error, setError] = useState<string>("");
  
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const streamRef = useRef<MediaStream | null>(null);
  const intervalRef = useRef<NodeJS.Timeout | null>(null);
  const finalTranscriptRef = useRef<string>("");
  const isRecordingRef = useRef<boolean>(false);

  // æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦æ”¯æŒ SpeechRecognition API
  useEffect(() => {
    const SpeechRecognition = 
      (window as any).SpeechRecognition || 
      (window as any).webkitSpeechRecognition;
    
    if (SpeechRecognition) {
      setIsSupported(true);
    } else {
      setIsSupported(false);
      setError("æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½ã€‚è¯·ä½¿ç”¨ Chromeã€Edge æˆ– Safari 14.1+ æµè§ˆå™¨ã€‚");
    }
  }, []);

  useEffect(() => {
    return () => {
      // æ¸…ç†èµ„æº
      if (recognitionRef.current) {
        recognitionRef.current.stop();
        recognitionRef.current = null;
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
      }
      if (intervalRef.current) {
        clearInterval(intervalRef.current);
      }
    };
  }, []);

  const startRecording = async () => {
    if (!isSupported) {
      alert("æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½");
      return;
    }

    try {
      // åˆå§‹åŒ– SpeechRecognition
      const SpeechRecognition = 
        (window as any).SpeechRecognition || 
        (window as any).webkitSpeechRecognition;
      
      const recognition = new SpeechRecognition();
      recognitionRef.current = recognition;

      // é…ç½®è¯†åˆ«å‚æ•°
      recognition.continuous = true; // æŒç»­è¯†åˆ«
      recognition.interimResults = true; // è¿”å›ä¸´æ—¶ç»“æœ
      recognition.lang = "zh-CN"; // è®¾ç½®ä¸ºä¸­æ–‡

      // å¼€å§‹å½•éŸ³ï¼ˆå¯é€‰ï¼Œç”¨äºä¿å­˜éŸ³é¢‘ï¼‰
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;

      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: "audio/webm" });
        const audioUrl = URL.createObjectURL(audioBlob);
        
        const record: VoiceRecord = {
          id: `voice-${Date.now()}`,
          type: "note",
          status: "completed",
          audioUrl,
          duration,
          transcript: finalTranscriptRef.current,
          confidence: 0.9, // SpeechRecognition ä¼šæä¾›ç½®ä¿¡åº¦
          createdAt: new Date().toISOString(),
          updatedAt: new Date().toISOString(),
        };

        if (onRecordComplete) {
          onRecordComplete(record);
        }
        if (onTranscript && finalTranscriptRef.current) {
          onTranscript(finalTranscriptRef.current);
        }
      };

      // è¯­éŸ³è¯†åˆ«äº‹ä»¶å¤„ç†
      recognition.onstart = () => {
        isRecordingRef.current = true;
        setIsRecording(true);
        setStatus("recording");
        setDuration(0);
        setTranscript("");
        setInterimTranscript("");
        finalTranscriptRef.current = "";
        setError("");
        mediaRecorder.start();
      };

      recognition.onresult = (event: SpeechRecognitionEvent) => {
        let interimTranscript = "";
        let finalTranscript = "";

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript + " ";
          } else {
            interimTranscript += transcript;
          }
        }

        if (finalTranscript) {
          finalTranscriptRef.current += finalTranscript;
          setTranscript(finalTranscriptRef.current);
          setInterimTranscript("");
        } else {
          setInterimTranscript(interimTranscript);
        }

        // å®æ—¶å›è°ƒ
        if (onTranscript && finalTranscript) {
          onTranscript(finalTranscriptRef.current);
        }
      };

      recognition.onerror = (event: any) => {
        console.error("Speech recognition error:", event.error);
        setError(`è¯†åˆ«é”™è¯¯: ${event.error}`);
        if (event.error === "no-speech") {
          // æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå¯ä»¥ç»§ç»­ç­‰å¾…
          return;
        }
        stopRecording();
      };

      recognition.onend = () => {
        // å¦‚æœè¿˜åœ¨å½•éŸ³çŠ¶æ€ï¼Œè‡ªåŠ¨é‡å¯ï¼ˆå®ç°è¿ç»­è¯†åˆ«ï¼‰
        if (isRecordingRef.current && recognitionRef.current) {
          try {
            recognitionRef.current.start();
          } catch (e) {
            // å¦‚æœå¯åŠ¨å¤±è´¥ï¼Œåœæ­¢å½•éŸ³
            stopRecording();
          }
        }
      };

      // å¼€å§‹è¯†åˆ«
      recognition.start();
      setStatus("recording");

      // è®¡æ—¶å™¨
      intervalRef.current = setInterval(() => {
        setDuration((prev) => prev + 1);
      }, 1000);
    } catch (error) {
      console.error("Error accessing microphone:", error);
      setError("æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®");
      alert("æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®");
    }
  };

  const stopRecording = () => {
    isRecordingRef.current = false;
    
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      recognitionRef.current = null;
    }
    
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop();
    }
    
    setIsRecording(false);
    setStatus("completed");
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
    }
    
    if (intervalRef.current) {
      clearInterval(intervalRef.current);
      intervalRef.current = null;
    }
  };

  const formatDuration = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}`;
  };

  return (
    <div className="rounded-lg border border-border bg-card p-6">
      <div className="flex flex-col items-center gap-4">
        <h3 className="text-lg font-semibold">è¯­éŸ³è®°å½•</h3>
        
        {/* å½•éŸ³çŠ¶æ€æ˜¾ç¤º */}
        <div className="flex flex-col items-center gap-2">
          {isRecording && (
            <div className="flex items-center gap-2 text-destructive">
              <span className="h-3 w-3 animate-pulse rounded-full bg-destructive"></span>
              <span className="text-sm font-medium">å½•éŸ³ä¸­...</span>
            </div>
          )}
          {status === "processing" && (
            <div className="flex items-center gap-2 text-secondary-600">
              <span className="text-sm font-medium">å¤„ç†ä¸­...</span>
            </div>
          )}
          {duration > 0 && (
            <div className="text-2xl font-mono font-bold text-foreground">
              {formatDuration(duration)}
            </div>
          )}
        </div>

        {/* å½•éŸ³æŒ‰é’® */}
        <div className="flex gap-4">
          {!isRecording ? (
            <Button
              onClick={startRecording}
              variant="default"
              size="lg"
              className="gap-2"
            >
              <span className="text-xl">ğŸ¤</span>
              <span>å¼€å§‹å½•éŸ³</span>
            </Button>
          ) : (
            <Button
              onClick={stopRecording}
              variant="destructive"
              size="lg"
              className="gap-2"
            >
              <span className="text-xl">â¹</span>
              <span>åœæ­¢å½•éŸ³</span>
            </Button>
          )}
        </div>

        {/* é”™è¯¯æç¤º */}
        {error && (
          <div className="mt-4 w-full rounded-md border border-destructive bg-destructive/10 p-4">
            <p className="text-sm text-destructive">{error}</p>
          </div>
        )}

        {/* æµè§ˆå™¨æ”¯æŒæç¤º */}
        {!isSupported && (
          <div className="mt-4 w-full rounded-md border border-secondary-400 bg-secondary-50 p-4">
            <p className="text-sm text-secondary-800">
              âš ï¸ æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½
            </p>
            <p className="mt-1 text-xs text-secondary-700">
              è¯·ä½¿ç”¨ Chromeã€Edge æˆ– Safari 14.1+ æµè§ˆå™¨ä»¥è·å¾—æœ€ä½³ä½“éªŒ
            </p>
          </div>
        )}

        {/* è½¬å½•ç»“æœæ˜¾ç¤º */}
        {(transcript || interimTranscript) && (
          <div className="mt-4 w-full rounded-md border border-border bg-muted p-4">
            <p className="text-sm text-muted-foreground">è½¬å½•ç»“æœï¼š</p>
            {transcript && (
              <p className="mt-2 text-foreground">{transcript}</p>
            )}
            {interimTranscript && (
              <p className="mt-2 text-muted-foreground italic">
                {interimTranscript}
                <span className="ml-1 animate-pulse">|</span>
              </p>
            )}
          </div>
        )}
      </div>
    </div>
  );
}

